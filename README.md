# ArxivLlama ðŸ¦™ðŸ“š  
*A RAG-powered Scientific Research Assistant for ArXiv Papers*

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue)]()  
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()  
[![Streamlit](https://img.shields.io/badge/Interface-Streamlit-FF4B4B)]()

**ArxivLlama** transforms the way you explore and understand scientific literature by integrating Retrieval-Augmented Generation (RAG), natural language interfaces, and semantic search. This tool enables researchers to **search, analyze, summarize, and cite ArXiv papers with ease**.

![UI ArxivLlama Interface](images/image1.png)

---

## âœ¨ Features

### ðŸ“„ Paper Discovery
- ðŸ” Search and download papers from ArXiv by topic  
- ðŸ“¥ Automatically extract metadata (title, authors, abstract)  
- âš¡ Supports batch downloads (up to 50 papers at a time)  

### ðŸ§  Intelligent Analysis
- ðŸ¦™ RAG-powered semantic understanding using **LlamaIndex**  
- ðŸ’¬ Query paper content in natural language  
- ðŸ“Š Retrieve responses with relevance-based citations  
- ðŸ“– Get contextual excerpts from the source documents  

![Question Interface](images/image2.png)

### ðŸ› ï¸ Research Tools
- ðŸ“‘ APA-style citation generator  
- ðŸ“š AI-driven paper recommendations  
- ðŸ’° Cost tracking for OpenAI API calls  
- ðŸ—‚ï¸ Persistent vector storage via **ChromaDB**  

## Installation & Setup
# Clone the repository
git clone https://github.com/devericklasco/Scientific-Research-Assistant-ArxivLlama.git
cd Scientific-Research-Assistant-ArxivLlama

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate       # Linux/MacOS
.\.venv\Scripts\activate        # Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env

# Run the Streamlit app
streamlit run app.py

### Usage Guide
## Step 1: Download Papers
Input your research topic (e.g., "reinforcement learning in robotics")

Choose the number of papers to download (1â€“50)

Downloads saved to: data/papers/
![Download Interface](images/download.png)
![Download Interface](images/download2.png)  


## Step 2: Process PDFs
Automatically extract and chunk text semantically

Outputs stored in: data/chunks/ as .json
![ProcessPDF Interface](images/processpdf.png) 

## Step 3: Create Vector Index
Builds a semantic index from paper chunks

Uses OpenAI embeddings with optional cost tracking

Output: searchable knowledge base
![Create Vector Interface](images/createvector2.png)
![Create Vector Interface](images/createvector.png)

## Step 4: Query Research Papers
Ask questions like:
Get responses with source citations and context
![Questions Interface](images/questions.png)
APA citations are included
![Ask Interface](images/citations.png)

## Step 5: Get Paper Recommendations
Based on semantic similarity to your questions or uploaded content

Helps discover relevant papers within your indexed library

![Recommendations Interface](images/recommendation.png)


## Contact  
Built with â¤ï¸ by [Erick Busuulwa](https://github.com/devericklasco)  
Follow me on [Twitter](https://x.com/ericklasco)  


---

## ðŸ§± Tech Stack

```mermaid
graph LR
A[ArXiv API] --> B[PDF Processing]
B --> C[Text Chunking]
C --> D[Embedding Generation]
D --> E[Vector DB: ChromaDB]
E --> F[Query Engine: LlamaIndex]
F --> G[LLM Interface: OpenAI]
G --> H[Streamlit UI]

